{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"T_bert_train.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xeJuZT92jQj","executionInfo":{"status":"ok","timestamp":1616656446494,"user_tz":-330,"elapsed":11227,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"011afd78-eb2c-44fd-95f6-a8049d3896a8"},"source":["!pip install tez\n","!pip install torch==1.7.1\n","!pip install transformers==3.5.1\n","!pip install autocorrect"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tez in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tez) (1.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->tez) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->tez) (3.7.4.3)\n","Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (1.7.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.5)\n","Requirement already satisfied: transformers==3.5.1 in /usr/local/lib/python3.7/dist-packages (3.5.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (20.9)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (0.0.43)\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (0.1.91)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.12.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (3.0.12)\n","Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (0.9.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.1) (2019.12.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.1) (2020.12.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.5.1) (54.1.2)\n","Collecting autocorrect\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/a8/1fc332535fc26db807fa48bdb54070355b83a36c797451c3d563bc190fa8/autocorrect-2.3.0.tar.gz (621kB)\n","\u001b[K     |████████████████████████████████| 624kB 8.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: autocorrect\n","  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autocorrect: filename=autocorrect-2.3.0-cp37-none-any.whl size=621587 sha256=5d94b837738d3c966eeb00d9003925c4f9c1c270c10d4ce6235be3087643da20\n","  Stored in directory: /root/.cache/pip/wheels/cc/1c/30/6b0199afbd20eef5959f5eaa0ead86aeef84391740482b2279\n","Successfully built autocorrect\n","Installing collected packages: autocorrect\n","Successfully installed autocorrect-2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0UGidVj92hCN","executionInfo":{"status":"ok","timestamp":1616658452091,"user_tz":-330,"elapsed":12114,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}}},"source":["import pandas as pd\n","import tez\n","import torch\n","import torch.nn as nn\n","import transformers\n","from sklearn import metrics, model_selection, preprocessing\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from sklearn.utils import shuffle\n","from sklearn import model_selection\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwxEB7o-RtT1","executionInfo":{"status":"ok","timestamp":1616658452096,"user_tz":-330,"elapsed":11868,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"546924d2-6a63-4c92-b57a-674b434a7a25"},"source":["drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/MyDrive/Work/Dan/BERT Model 2')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AoH_ZsW02MNJ","executionInfo":{"status":"ok","timestamp":1616658452097,"user_tz":-330,"elapsed":11613,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}}},"source":["class BERTDataset:\n","    def __init__(self, text, target):\n","        self.text = text\n","        self.target = target\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n","            \"bert-large-uncased\", do_lower_case=True\n","        )\n","        self.max_len = 64\n","    def __len__(self):\n","        return len(self.text)\n","    def __getitem__(self, item):\n","        text = str(self.text[item])\n","        text = \" \".join(text.split())\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            truncation=True,\n","        )\n","        ids = inputs[\"input_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","        padding_length = self.max_len - len(ids)\n","        \n","        ids = ids + ([0] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","\n","        return {\n","            \"ids\": torch.tensor(ids, dtype=torch.long),\n","            \"mask\": torch.tensor(mask, dtype=torch.long),\n","            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n","            \"targets\": torch.tensor(self.target[item], dtype=torch.long),\n","        }\n","\n","class BERTBaseUncased(tez.Model):\n","    def __init__(self, num_train_steps, num_classes):\n","        super().__init__()\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n","            \"bert-large-uncased\", do_lower_case=True\n","        )\n","        self.bert = transformers.BertModel.from_pretrained(\"bert-large-uncased\")\n","        self.bert_drop = nn.Dropout(0.3)\n","        self.out = nn.Linear(1024, num_classes)\n","        self.num_train_steps = num_train_steps\n","        self.step_scheduler_after = \"batch\"\n","    def fetch_optimizer(self):\n","        param_optimizer = list(self.named_parameters())\n","        no_decay = [\"bias\", \"LayerNorm.bias\"]\n","        optimizer_parameters = [\n","            {\n","                \"params\": [\n","                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n","                ],\n","                \"weight_decay\": 0.001,\n","            },\n","            {\n","                \"params\": [\n","                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n","                ],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        opt = AdamW(optimizer_parameters, lr=3e-5)\n","        return opt\n","    def fetch_scheduler(self):\n","        sch = get_linear_schedule_with_warmup(\n","            self.optimizer, num_warmup_steps=0, num_training_steps=self.num_train_steps\n","        )\n","        return sch\n","    def loss(self, outputs, targets):\n","        if targets is None:\n","            return None\n","        return nn.CrossEntropyLoss()(outputs, targets)\n","    def monitor_metrics(self, outputs, targets):\n","        if targets is None:\n","            return {}\n","        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n","        targets = targets.cpu().detach().numpy()\n","        accuracy = metrics.accuracy_score(targets, outputs)\n","        return {\"accuracy\": accuracy}\n","    def forward(self, ids, mask, token_type_ids, targets=None):\n","        _, o_2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n","        b_o = self.bert_drop(o_2)\n","        output = self.out(b_o)\n","        loss = self.loss(output, targets)\n","        acc = self.monitor_metrics(output, targets)\n","        return output, loss, acc\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPcYIu0BYsBx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ce47XBCVLqEO","executionInfo":{"status":"ok","timestamp":1616659014287,"user_tz":-330,"elapsed":1613,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}}},"source":["# df_train=pd.read_excel('Transcript-comb.xlsx',sheet_name='Traindata-Hiri')\n","# df_test=pd.read_excel('Transcript-comb.xlsx',sheet_name='Testdata-Hiri').rename(columns={'Input Conversation':'utterance'})\n","# from sklearn import preprocessing\n","# le = preprocessing.LabelEncoder()\n","# df_train['intent1']=le.fit_transform(df_train['intent'])\n","# import string\n","# import nltk\n","# nltk.download('stopwords')\n","# nltk.download('averaged_perceptron_tagger')\n","# nltk.download('wordnet')\n","# import re\n","\n","# def remove_punct(text):\n","#     text=text.lower()\n","#     text  = \"\".join([char for char in text if char not in string.punctuation])\n","#     return text\n","\n","# stopword = nltk.corpus.stopwords.words('english')\n","\n","# def remove_stopwords(text):\n","#     text = [word for word in re.split('\\W+', text) if ((word not in stopword)&(word!=''))]\n","#     return text\n","\n","# wn = nltk.WordNetLemmatizer()\n","\n","# def lemmatizer(text):\n","#     text = [wn.lemmatize(word) for word in text]\n","#     return text\n","\n","# from autocorrect import Speller\n","# spell = Speller(lang='en')\n","\n","# def spell_check(text):\n","#   return [spell(i) for i in text]\n","\n","# def clean_data(x):\n","#     x=x.encode('ascii','ignore').decode() # remove texts other than english\n","#     x=remove_punct(x) # remove punctuations\n","#     x=remove_stopwords(x) # remove stopwords\n","#     x=spell_check(x)\n","#     x=lemmatizer(x) # lemmatization\n","#     excluded_tags = [\"ADJ\", \"ADV\"]\n","#     t=nltk.pos_tag(x)\n","#     x=[i[0] for i in t if i[1] not in excluded_tags]\n","#     return ' '.join(x)\n","# clean_data(df_train.utterance.iloc[0])\n","# df_train['utterance']=df_train['utterance'].apply(lambda x: clean_data(x))\n","# df_test['utterance']=df_test['utterance'].astype(str).apply(lambda x: clean_data(x))\n","# df_train.to_csv('train_data.csv',index=None)\n","# df_test.to_csv('test_data.csv',index=None)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Bk9hgi63I9D"},"source":["if __name__ == \"__main__\":\n","\n","    df_train=pd.read_csv('train_data.csv')\n","    df_test=pd.read_csv('test_data.csv')\n","\n","    X_train, X_valid, y_train, y_valid = train_test_split(df_train['utterance'], df_train['intent1'], test_size=0.2, random_state=42)\n","    train_dataset = BERTDataset(text=X_train.values, target=y_train.values)\n","    valid_dataset = BERTDataset(text=X_valid.values, target=y_valid.values)\n","    n_train_steps = int(len(df_train) / 32 * 10)\n","    model = BERTBaseUncased(num_train_steps=n_train_steps, num_classes=df_train.intent1.nunique())\n","    train_dataset = BERTDataset(text=X_train.values, target=y_train.values)\n","    valid_dataset = BERTDataset(text=X_valid.values, target=y_valid.values)\n","    n_train_steps = int(len(df_train) / 32 * 10)\n","    model = BERTBaseUncased(num_train_steps=n_train_steps, num_classes=df_train.intent1.nunique())\n","    tb_logger = tez.callbacks.TensorBoardLogger(log_dir=\".logs/\")\n","    es = tez.callbacks.EarlyStopping(monitor=\"valid_loss\", model_path=\"model.bin\")\n","    model.fit(train_dataset,valid_dataset=valid_dataset,train_bs=32, device='cuda',epochs=1,callbacks=[tb_logger,es],fp16=True)\n","\n","    torch.save(model.state_dict(), \"model.bin\")\n","    #preds = model.predict(valid_dataset, batch_size=16, n_jobs=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aXMawb__ZBHb","executionInfo":{"status":"ok","timestamp":1616659225709,"user_tz":-330,"elapsed":1191,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}}},"source":["# lmap = dict(zip(le.transform(le.classes_),le.classes_))\n","# import pickle\n","# with open('lmap.pkl','wb') as f:\n","#   pickle.dump(lmap,f)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pk5JkTHfZATC"},"source":[""],"execution_count":null,"outputs":[]}]}