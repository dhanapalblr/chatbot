{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('templates'):\n",
    "    os.mkdir('templates')\n",
    "    \n",
    "if not os.path.exists('static'):\n",
    "    os.mkdir('static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting templates/home.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile templates/home.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "\t<title>Home</title>\n",
    "\t<!-- <link rel=\"stylesheet\" type=\"text/css\" href=\"../static/css/styles.css\"> -->\n",
    "\t<link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='css/styles.css') }}\">\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "\t<header>\n",
    "\t\t<div class=\"container\">\n",
    "\t\t<div id=\"brandname\">\n",
    "\t\t\tAn app built with Flask\n",
    "\t\t</div>\n",
    "\t\t<h2>Classification</h2>\n",
    "\t\t\n",
    "\t</div>\n",
    "\t</header>\n",
    "\n",
    "\t<div class=\"ml-container\">\n",
    "\n",
    "\t\t<form action=\"{{ url_for('SimilarUtterances')}}\" method=\"POST\">\n",
    "\t\t<p>Enter the Input here</p>\n",
    "\t\t<!-- <input type=\"text\" name=\"comment\"/> -->\n",
    "\t\t<textarea name=\"message\" rows=\"4\" cols=\"50\"></textarea>\n",
    "\t\t<br/>\n",
    "\n",
    "\t\t<input type=\"submit\" class=\"btn-info\" value=\"SimilarUtterances\">\n",
    "\t\t\n",
    "\t</form>\n",
    "\t\t\n",
    "\t</div>\n",
    "\n",
    "\t\n",
    "\t\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing templates/result.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile templates/result.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "\t<title></title>\n",
    "    <link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='css/styles.css') }}\">\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "\t<header>\n",
    "\t\t<div class=\"container\">\n",
    "\t\t<div id=\"brandname\">\n",
    "\t\t\tML App\n",
    "\t\t</div>\n",
    "\t\t<h2>Similar Intents</h2>\n",
    "\t\t\n",
    "\t</div>\n",
    "\t</header>\n",
    "\t<div class=\"results\">\n",
    "\n",
    "\n",
    "\t<h3 style=\"color:black;\">{{prediction}}</h3>\n",
    "\n",
    "\t</div>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting static/styles.css\n"
     ]
    }
   ],
   "source": [
    "%%writefile static/styles.css\n",
    "body{\n",
    "\tfont:15px/1.5 Arial, Helvetica,sans-serif;\n",
    "\tpadding: 0px;\n",
    "\tbackground-color:#f4f3f3;\n",
    "}\n",
    "\n",
    ".container{\n",
    "\twidth:100%;\n",
    "\tmargin: auto;\n",
    "\toverflow: hidden;\n",
    "}\n",
    "\n",
    "header{\n",
    "\tbackground:#03A9F4;#35434a;\n",
    "\tborder-bottom:#448AFF 3px solid;\n",
    "\theight:120px;\n",
    "\twidth:100%;\n",
    "\tpadding-top:30px;\n",
    "\n",
    "}\n",
    "\n",
    ".main-header{\n",
    "\t\t\ttext-align:center;\n",
    "\t\t\tbackground-color: blue;\n",
    "\t\t\theight:100px;\n",
    "\t\t\twidth:100%;\n",
    "\t\t\tmargin:0px;\n",
    "\t\t}\n",
    "#brandname{\n",
    "\tfloat:left;\n",
    "\tfont-size:30px;\n",
    "\tcolor: #fff;\n",
    "\tmargin: 10px;\n",
    "}\n",
    "\n",
    "header h2{\n",
    "\ttext-align:center;\n",
    "\tcolor:#fff;\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    ".btn-info {background-color: #2196F3;\n",
    "\theight:40px;\n",
    "\twidth:100px;} /* Blue */\n",
    ".btn-info:hover {background: #0b7dda;}\n",
    "\n",
    "\n",
    ".resultss{\n",
    "\tborder-radius: 15px 50px;\n",
    "    background: #345fe4;\n",
    "    padding: 20px; \n",
    "    width: 200px;\n",
    "    height: 150px;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "import pandas as pd\n",
    "# import tez\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import transformers\n",
    "# from sklearn import metrics, model_selection, preprocessing\n",
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "# from sklearn.utils import shuffle\n",
    "# import torch.nn.functional as nnf\n",
    "import json\n",
    "import os\n",
    "from flask import Flask, request, jsonify, escape, url_for\n",
    "import requests\n",
    "# from waitress import serve\n",
    "from collections import Counter\n",
    "import logging\n",
    "import datetime as dt\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import socket\n",
    "import jieba\n",
    "import enum\n",
    "\n",
    "DATA_DIR = './'\n",
    "\n",
    "def spl_char_remove(text):\n",
    "    split_string=text.split(\" \")\n",
    "    total=[]\n",
    "    for i in range(0,len(split_string)):\n",
    "        no=''.join([*filter(str.isalnum,split_string[i])])\n",
    "        total.append(str(no.strip()))\n",
    "    listToStr = ' '.join([str(elem) for elem in total]) \n",
    "    return listToStr\n",
    "#     print(listToStr)\n",
    "spl_char_remove(\"jadjk12 jdakj %$ sajjh\")\n",
    "\n",
    "def load_df(json_path='name.json'):\n",
    "    \"\"\"\n",
    "    source: borrowed to kaggle competition gstore\n",
    "    \"\"\"\n",
    "    df = pd.read_json(DATA_DIR+json_path)\n",
    "    \n",
    "    for column in ['Issues']:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [str(column+\"_\"+subcolumn) for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    \n",
    "    ## function allows to keep the index if we need to merge on the orginal data.\n",
    "    df = pd.DataFrame([dict(y, index=i) for i, x in enumerate(df['Issues_Messages'].values.tolist()) for y in x])\n",
    "    \n",
    "    #print(df.shape)\n",
    "    return df\n",
    "\n",
    "def splitDataFrameList(df,target_column,separator):\n",
    "    \n",
    "    ''' \n",
    "    source: https://gist.github.com/jlln/338b4b0b55bd6984f883 modified to keep punctuation\n",
    "    df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    def split_text(line, separator):\n",
    "        splited_line =  [e+d for e in line.split(separator) if e]\n",
    "        return splited_line\n",
    "    \n",
    "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "        split_row = row[target_column].split(separator)\n",
    "        for s in split_row:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = s\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows,axis=1,args = (new_rows,target_column,separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "class Autocompleter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def import_json(self, json_filename):\n",
    "        #print(\"load json file...\")\n",
    "        df = load_df(json_filename)\n",
    "        return df\n",
    "        \n",
    "    def process_data(self, new_df):\n",
    "        #print(\"select representative threads...\")\n",
    "        new_df = new_df[new_df.IsFromCustomer==False]\n",
    "        \n",
    "        #print(\"split sentenses on punctuation...\")\n",
    "        for sep in ['. ',', ','? ', '! ', '; ']:\n",
    "            new_df = splitDataFrameList(new_df, 'intent', sep)\n",
    "            \n",
    "        #print(\"Text Cleaning using simple regex...\")\n",
    "        new_df['intent']=new_df['intent'].apply(lambda x: \" \".join(x.split()))\n",
    "        new_df['intent']=new_df['intent'].apply(lambda x: x.strip(\".\"))\n",
    "        new_df['intent']=new_df['intent'].apply(lambda x: \" \".join(x.split()))\n",
    "        new_df['intent']=new_df['intent'].apply(lambda x: x.replace(' i ',' I '))\n",
    "        new_df['intent']=new_df['intent'].apply(lambda x: x.replace(' ?','?'))\n",
    "        new_df['intent']=new_df['intent'].apply(lambda x: x.replace(' !','!'))\n",
    "        new_df['intent']=new_df['intent'].apply(lambda x: x.replace(' .','.'))\n",
    "        new_df['intent']=new_df['intent'].apply(lambda x: x.replace('OK','Ok'))\n",
    "#         new_df['intent']=new_df['intent'].apply(lambda x: x[0].upper()+x[1:])\n",
    "        new_df['intent']=new_df['intent'].apply(lambda x: x+\"?\" if re.search(r'^(Wh|How).+([^?])$',x) else x)\n",
    "        \n",
    "        print(\"calculate nb words of sentenses...\")\n",
    "        new_df['nb_words'] = new_df['intent'].apply(lambda x: len(str(x).split(' ')))\n",
    "        new_df = new_df[new_df['nb_words']>2]\n",
    "        \n",
    "        print(\"count occurence of sentenses...\")\n",
    "        new_df['Counts'] = new_df.groupby(['intent'])['intent'].transform('count')\n",
    "        \n",
    "        print(\"remove duplicates (keep last)...\")\n",
    "        new_df = new_df.drop_duplicates(subset=['intent'], keep='last')\n",
    "        \n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "        print(new_df.shape)  \n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    def calc_matrice(self, df):\n",
    "        # define tfidf parameter in order to count/vectorize the description vector and then normalize it.\n",
    "        model_tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 10), min_df=0)\n",
    "        tfidf_matrice = model_tf.fit_transform(df['intent'])\n",
    "        #print(\"tfidf_matrice \", tfidf_matrice.shape)\n",
    "        return model_tf, tfidf_matrice\n",
    "    def generate_completions(self, prefix_string, data, model_tf, tfidf_matrice):\n",
    "        \n",
    "        prefix_string = str(prefix_string)\n",
    "        new_df = data.reset_index(drop=True)\n",
    "        weights = new_df['Counts'].apply(lambda x: 1+ np.log1p(x)).values\n",
    "        # tranform the string using the tfidf model\n",
    "        tfidf_matrice_spelling = model_tf.transform([prefix_string])\n",
    "        # calculate cosine_matrix\n",
    "        cosine_similarite = linear_kernel(tfidf_matrice, tfidf_matrice_spelling)\n",
    "        \n",
    "        #sort by order of similarity from 1 to 0:\n",
    "        similarity_scores = list(enumerate(cosine_similarite))\n",
    "        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "        similarity_scores = similarity_scores[0:100]\n",
    "        similarity_scores = [i for i in similarity_scores]\n",
    "        similarity_indices = [i[0] for i in similarity_scores]\n",
    "        #add weight to the potential results that had high frequency in orig data\n",
    "        for i in range(len(similarity_scores)):\n",
    "            similarity_scores[i][1][0]=similarity_scores[i][1][0]*weights[similarity_indices][i]\n",
    "        similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "        similarity_scores = similarity_scores[0:3] # selecting top 3 intnets\n",
    "        similarity_indices_w = [i[0] for i in similarity_scores]\n",
    "        \n",
    "        return new_df.loc[similarity_indices_w]['intent'].tolist(),similarity_scores\n",
    "import re\n",
    "def space_remove(text):\n",
    "    text=re.sub(' +', ' ',text)\n",
    "    return text\n",
    "\n",
    "autocompl = Autocompleter()\n",
    "df1=pd.read_excel(\"acoe.xlsx\")\n",
    "df1.rename(columns={\"intent\": \"intent1\"},inplace=True)\n",
    "df1[\"intent\"]=df1.intent1.str.lower()\n",
    "df1[\"intent\"]=df1[\"intent\"].str.strip()\n",
    "\n",
    "import re\n",
    "def clean_text(text):\n",
    "#     text=re.sub('\\w*\\d\\w*','', text)\n",
    "    text=re.sub('\\n',' ',text)\n",
    "    text=re.sub(r\"http\\S+\", \"\", text)\n",
    "    text=re.sub('[^A-Za-z0-9]',' ',text)\n",
    "    return text\n",
    "\n",
    "df1[\"intent\"]=df1[\"intent\"].apply(lambda x: clean_text(x))\n",
    "df1[\"intent\"]=df1[\"intent\"].apply(spl_char_remove)\n",
    "df1[\"intent\"]=df1[\"intent\"].apply(space_remove)\n",
    "df1[\"intent\"]=df1[\"intent\"].str.strip()\n",
    "\n",
    "df1.to_excel(\"acoes.xlsx\")\n",
    "# print(df1.head(5))\n",
    "# print(df1[\"Text\"][0])\n",
    "df1=pd.read_excel('acoes.xlsx')\n",
    "df=df1[[\"intent\",\"IsFromCustomer\"]]\n",
    "df[\"intent\"]=df[\"intent\"].astype(str).str.strip()\n",
    "new_df = autocompl.process_data(df)\n",
    "model_tf, tfidf_matrice = autocompl.calc_matrice(new_df)\n",
    "dftxt=df.intent.values\n",
    "\n",
    "def SimUtter(prefix):\n",
    "    prefix=str(prefix).lower()\n",
    "    #print(prefix)\n",
    "    similar_uttrances,utterance_similarity_score=autocompl.generate_completions(prefix, new_df, model_tf,tfidf_matrice)\n",
    "    return similar_uttrances,utterance_similarity_score\n",
    "\n",
    "def logit(logs):\n",
    "    date_today = dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "    #logging.basicConfig(filename= BASE_PATH + 'hiri/logs/intentProcessor-hiri-'+ str(date_today)+ '.log', level=logging.INFO)\n",
    "    #logging.info(format(dt.datetime.now()) +\" \"+ logs)\n",
    "    \n",
    "\n",
    "import os\n",
    "from flask import Flask, render_template, request\n",
    "from flask import Flask\n",
    "from string import Template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "\treturn render_template('home.html')\n",
    "    \n",
    "@app.route('/similarutterances', methods = ['POST'])\n",
    "def SimilarUtterances():\n",
    "    #data = request.get_json()\n",
    "    data=request.form['message']\n",
    "    \n",
    "#     for handler in logging.root.handlers[:]:\n",
    "#         logging.root.removeHandler(handler)\n",
    "\n",
    "    logit(\"request received - \" + str(data))\n",
    "    prefix = data#data[\"prefix\"]\n",
    "    print(prefix)\n",
    "\n",
    "    similar_uttrances,utterance_similarity_score=SimUtter(prefix)\n",
    "    final_json = {\"similarutterancelist\": [\n",
    "                {\"similar_intents\": similar_uttrances,\n",
    "                 \"utterance_similarity_score\":utterance_similarity_score,\n",
    "                  }]}\n",
    "\n",
    "#     jsonData=json.dumps(final_json)\n",
    "#     logit(\"after predictions - \" + str(jsonData))\n",
    "#     response = app.response_class(\n",
    "#     response=jsonData,\n",
    "#     status=200,\n",
    "#     mimetype='application/json'\n",
    "#     )\n",
    "#     logit(\"response sent - \" + str(response))\n",
    "    \n",
    "    return render_template('result.html',prediction = final_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t#decide what port to run the app in\n",
    "\tport = int(os.environ.get('PORT', 5000))\n",
    "\t#run the app locally on the givn port\n",
    "\tapp.run(host='0.0.0.0', port=port)\n",
    "\t#optional if we want to run in debugging mode\n",
    "\t#app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py:197: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"intent\"]=df[\"intent\"].astype(str).str.strip()\n",
      "calculate nb words of sentenses...\n",
      "count occurence of sentenses...\n",
      "remove duplicates (keep last)...\n",
      "(1246, 4)\n",
      " * Serving Flask app \"app\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [19/Mar/2021 10:17:48] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2021 10:17:48] \"\u001b[33mGET /static/css/styles.css HTTP/1.1\u001b[0m\" 404 -\n",
      "aldhfslkdhflshdf\n",
      "127.0.0.1 - - [19/Mar/2021 10:17:58] \"\u001b[37mPOST /similarutterances HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [19/Mar/2021 10:17:58] \"\u001b[33mGET /static/css/styles.css HTTP/1.1\u001b[0m\" 404 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
