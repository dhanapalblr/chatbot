{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hihp8VYCMP33"
   },
   "source": [
    "### Load the libraries (change directories based on the system on which code is running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19214,
     "status": "ok",
     "timestamp": 1615817761976,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "x0mW-Ny56qod",
    "outputId": "009bef5f-f092-4f08-e738-b1aa8856abc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13605,
     "status": "ok",
     "timestamp": 1615817776546,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "D7O-Ek3TTqCs",
    "outputId": "d0efe808-9e52-4302-e2b2-90701451a2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 18.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 55.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 53.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=88224951242aefa8e5234d5b1427d9fda854750bbdcc4424026a1de549a47778\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "os.chdir('/content/drive/MyDrive/Work/Dan/BERT Model')\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "!pip install transformers\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3Dswby7sx40"
   },
   "source": [
    "## Prepare Datasets (uncomment if datasets are changing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYWWqDSFkHcf"
   },
   "source": [
    "use domain specific - for hiri, iris, and lcs - build 3 bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6240,
     "status": "ok",
     "timestamp": 1615817905332,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "ZMhK-nWOT0w7"
   },
   "outputs": [],
   "source": [
    "train=pd.read_excel('Input data/Model_training_all_06032021.xlsx',sheet_name='Sheet2').drop_duplicates().reset_index(drop=True)\n",
    "train=train[train['domain']=='LCS']\n",
    "train=train[train['language']=='EN'][['cantonese_text','intent']].rename(columns={'cantonese_text':'text','intent':'label'})\n",
    "test=pd.read_excel('Input data/Gold_data_all_06032021.xlsx',sheet_name='Sheet2').drop_duplicates().reset_index(drop=True)\n",
    "test=test[test['domain']=='LCS']\n",
    "test=test[test['language']=='EN'][['utterance','intentid_expected']].rename(columns={'utterance':'text','intentid_expected':'label'})\n",
    "test=test[test.label.isin(train.label)]\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in re.split('\\W+', text) if word not in stopword]\n",
    "    return text\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def spell_correction(text):           # spelling correction\n",
    "    txt=TextBlob(text)\n",
    "    return txt.correct()\n",
    "\n",
    "def clean_data(x):\n",
    "    x=x.lower()\n",
    "    x=x.encode('ascii','ignore').decode() # remove texts other than english\n",
    "    x=re.sub('https*\\S+','',x) # remove urls\n",
    "    #x=spell_correction(x)\n",
    "    x=remove_punct(x) # remove punctuations\n",
    "    x=remove_stopwords(x) # remove stopwords\n",
    "    #x=stemming(x) # stemming\n",
    "    #x=lemmatizer(x) # lemmatization\n",
    "    return ' '.join(x)\n",
    "\n",
    "\n",
    "train['text']=train['text'].apply(lambda x: clean_data(x))\n",
    "test['text']=test['text'].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5683,
     "status": "ok",
     "timestamp": 1615817905334,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "Y0Yg22C7V6uM"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train['label'])\n",
    "train['label']=le.transform(train['label'])\n",
    "test['label']=le.transform(test['label'])\n",
    "with open('model_files/label_encoder_lcs.pkl', 'wb') as handle:\n",
    "    pickle.dump(le, handle)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train,valid = train_test_split(train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JY09898_MjD-"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3995,
     "status": "ok",
     "timestamp": 1615817905842,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "9iW4sLBpstBN"
   },
   "outputs": [],
   "source": [
    "with open('model_files/label_encoder_lcs.pkl', 'rb') as handle:\n",
    "    le=pickle.load(handle)\n",
    "train_text,train_labels=train['text'],train['label']\n",
    "val_text,val_labels=valid['text'],valid['label']\n",
    "test_text,test_labels=test['text'],test['label']\n",
    "bert = AutoModel.from_pretrained('albert-base-v1')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "seq_len = [len(str(i).split()) for i in train_text]\n",
    "tokens_train = tokenizer.batch_encode_plus(train_text.astype(str).tolist(),max_length=25,truncation=True,pad_to_max_length=True)\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(val_text.astype(str).tolist(),max_length=25,truncation=True,pad_to_max_length=True)\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(test_text.astype(str).tolist(),max_length=25,truncation=True,pad_to_max_length=True)\n",
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYVMVwNoMlY6"
   },
   "source": [
    "## BERT model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1552,
     "status": "ok",
     "timestamp": 1615817905843,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "psuq_w7CG7V7"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 64\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi-KJmlAMq5O"
   },
   "source": [
    "## Pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1615817905844,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "mlNuUW19G_KO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,len(le.classes_))\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      #self.fc2 = nn.Linear(512,len(le.classes_))\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      k = self.bert(sent_id, attention_mask=mask)\n",
    "      cls_hs=k[1]\n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      #x = self.relu(x)\n",
    "\n",
    "      #x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      #x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 11761,
     "status": "ok",
     "timestamp": 1615817918143,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "kOJ7lL8gHEl6"
   },
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 0.01)          # learning rate\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss() \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNbsqaauMy3M"
   },
   "source": [
    "## Training of BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7995,
     "status": "ok",
     "timestamp": 1615817918146,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "eO_qg60bHN5G"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  correct=0\n",
    "  total=0\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    # if step % 100 == 0 and not step == 0:\n",
    "    #   print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    correct += (torch.max(preds, 1)[1] == labels).float().sum()\n",
    "    total += labels.shape[0]\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds, correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIxBhz37M2me"
   },
   "source": [
    "## Evaluate the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1615817918147,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "vPspriySHVSP"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "  correct=0\n",
    "  total=0\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    #if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "            \n",
    "      # Report progress.\n",
    "      #print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "      correct += (torch.max(preds, 1)[1] == labels).float().sum()\n",
    "      total += labels.shape[0]\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds ,correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRaPXU5CM8Xc"
   },
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "executionInfo": {
     "elapsed": 41386,
     "status": "error",
     "timestamp": 1615817960220,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "pqLsSn5CHdTR",
    "outputId": "921f50e9-d217-456b-9f5c-fe8cf0a94bb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 5.990, Training Accuracy:  0.091\n",
      "Validation Loss: 5.040, Validation Accuracy:  0.177\n",
      "\n",
      " Epoch 2 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 4.638, Training Accuracy:  0.178\n",
      "Validation Loss: 4.018, Validation Accuracy:  0.290\n",
      "\n",
      " Epoch 3 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 4.342, Training Accuracy:  0.227\n",
      "Validation Loss: 3.930, Validation Accuracy:  0.330\n",
      "\n",
      " Epoch 4 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 4.235, Training Accuracy:  0.239\n",
      "Validation Loss: 3.666, Validation Accuracy:  0.336\n",
      "\n",
      " Epoch 5 / 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f5512e2e5bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fbc38702d31d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# add on to the total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# backward pass to calculate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _, train_acc = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _, valid_acc = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "\n",
    "    torch.save(model.state_dict(), 'model_files/saved_weights_lcs.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}, Training Accuracy: {train_acc: .3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}, Validation Accuracy: {valid_acc: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1615817963998,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "R1n_b8PATGWs",
    "outputId": "eb83ca5b-5675-4872-afa2-333a36b23ed0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'model_files/saved_weights_lcs.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2257,
     "status": "ok",
     "timestamp": 1615817965638,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "WKsqmXwSHgkb",
    "outputId": "454d98a0-d04e-4af6-fafd-46fa492ff8e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22533136966126657"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq.to(device), test_mask.to(device))\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "test['predicted']=le.inverse_transform(preds)\n",
    "test['actual']=le.inverse_transform(test['label'])\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test['actual'], test['predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1572,
     "status": "ok",
     "timestamp": 1615817965640,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "7rQ0Y0tiXewr"
   },
   "outputs": [],
   "source": [
    "test.to_csv('Predictions/predicted_lcs.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osHQsv4DXw8r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_lcs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
