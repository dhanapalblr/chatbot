{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Copy of LSTM classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35gDxPsaUgTx"
      },
      "source": [
        "## Import libraries and read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CTOU0CEBgpDj",
        "outputId": "5a2b6497-81da-4150-d2d5-dce7220d8461"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive') # mound the google drive\n",
        "# os.chdir('/content/drive/MyDrive/Work/Dan')\n",
        "\n",
        "import os\n",
        "!pip install jovian\n",
        "!pip install swifter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "#library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import spacy\n",
        "import jovian\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk \n",
        "import string\n",
        "import re\n",
        "import swifter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jovian\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e9/69d4efd1bd015e220526325bbfc872e2e50c18070ac8806e8300ff4046fb/jovian-0.2.32-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 26.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 30kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 51kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from jovian) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from jovian) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from jovian) (3.13)\n",
            "Collecting uuid\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/63/f42f5aa951ebf2c8dac81f77a8edcc1c218640a2a35a03b9ff2d4aa64c3d/uuid-1.30.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->jovian) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->jovian) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->jovian) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->jovian) (3.0.4)\n",
            "Building wheels for collected packages: uuid\n",
            "  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uuid: filename=uuid-1.30-cp36-none-any.whl size=6502 sha256=b788f1762f4719abc23ae2c8cc860c1af19a269f67ca2258fc36fd0a749a271c\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/80/9b/015026567c29fdffe31d91edbe7ba1b17728db79194fca1f21\n",
            "Successfully built uuid\n",
            "Installing collected packages: uuid, jovian\n",
            "Successfully installed jovian-0.2.32 uuid-1.30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "uuid"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting swifter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/3b/04bf42b94a22725241b47e0256458cde11f86f97572dd824e011f1ea8b20/swifter-1.0.7.tar.gz (633kB)\n",
            "\r\u001b[K     |▌                               | 10kB 28.1MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 13.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 12.3MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 11.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 143kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 174kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 204kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 225kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 235kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 256kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 266kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 286kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 307kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 327kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 348kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 358kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 368kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 378kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 399kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 409kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 419kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 430kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 440kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 450kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 460kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 471kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 481kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 491kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 501kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 512kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 522kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 532kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 542kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 552kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 563kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 573kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 583kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 593kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 604kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 614kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 624kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (1.1.5)\n",
            "Collecting psutil>=5.6.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/82/56cd16a4c5f53e3e5dd7b2c30d5c803e124f218ebb644ca9c30bc907eadd/psutil-5.8.0-cp36-cp36m-manylinux2010_x86_64.whl (291kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 15.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (2.12.0)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (4.41.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0cloudpickle>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from swifter) (7.6.3)\n",
            "Requirement already satisfied: parso>0.4.0 in /usr/local/lib/python3.6/dist-packages (from swifter) (0.8.1)\n",
            "Requirement already satisfied: bleach>=3.1.1 in /usr/local/lib/python3.6/dist-packages (from swifter) (3.3.0)\n",
            "Collecting modin[ray]>=0.8.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/9d/591b902668ef56fda55723c22cfe15a1986536d6c24cfbf65501ed9cf35e/modin-0.8.3-py3-none-manylinux1_x86_64.whl (564kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->swifter) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->swifter) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.0->swifter) (1.19.5)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.1)\n",
            "Collecting partd>=0.3.10; extra == \"dataframe\"\n",
            "  Downloading https://files.pythonhosted.org/packages/44/e1/68dbe731c9c067655bff1eca5b7d40c20ca4b23fd5ec9f3d17e201a6f36b/partd-1.1.0-py3-none-any.whl\n",
            "Collecting fsspec>=0.6.0; extra == \"dataframe\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.1.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (3.5.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.3.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach>=3.1.1->swifter) (20.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bleach>=3.1.1->swifter) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
            "Collecting ray>=1.0.0; extra == \"ray\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/e5/0ff593f053ff4fa2a582961272ef893c21d268d3c2c52ff1e7effd891e48/ray-1.1.0-cp36-cp36m-manylinux2014_x86_64.whl (48.5MB)\n",
            "\u001b[K     |████████████████████████████████| 48.5MB 66kB/s \n",
            "\u001b[?25hCollecting pyarrow==1.0; extra == \"ray\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/0a/a89de6d747c4698af128a46398703e3d1889f196478fd94a4e16bd1b5c65/pyarrow-1.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.2MB)\n",
            "\u001b[K     |████████████████████████████████| 17.2MB 216kB/s \n",
            "\u001b[?25hCollecting locket\n",
            "  Downloading https://files.pythonhosted.org/packages/50/b8/e789e45b9b9c2db75e9d9e6ceb022c8d1d7e49b2c085ce8c05600f90a96b/locket-0.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (53.0.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (4.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.6.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.3.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach>=3.1.1->swifter) (2.4.7)\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 46.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting gpustat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.4MB/s \n",
            "\u001b[?25hCollecting py-spy>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/b7/2056a6f06adb93f679f2a1e415dd33219b7c66ba69b8fd2ff1668b8064ed/py_spy-0.3.4-py2.py3-none-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.8MB/s \n",
            "\u001b[?25hCollecting aioredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.0.12)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.12.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.13)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.0.2)\n",
            "Collecting colorful\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (2.23.0)\n",
            "Collecting redis>=3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.6MB/s \n",
            "\u001b[?25hCollecting opencensus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/d6/b952f11b29c3a0cbec5620de3c4260cecd8c4329d83e91587edb48691e15/opencensus-0.7.12-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (0.9.0)\n",
            "Collecting aiohttp-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/13/e7/e436a0c0eb5127d8b491a9b83ecd2391c6ff7dcd5548dfaec2080a2340fd/aiohttp_cors-0.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (22.0.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.2.5)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.9.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (2.11.3)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (20.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (3.0.4)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 57.8MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (7.352.0)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
            "Collecting hiredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/7d/6acf1c8d4f2fb327ff6feec000b4c56a20628fbe966a4c7cd16c0b80343c/hiredis-1.1.0-cp36-cp36m-manylinux2010_x86_64.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (2.10)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.16.0)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/33/990f1bd9e7ee770fc8d3c154fc24743a96f16a0e49e14e1b7540cc2fdd93/opencensus_context-0.1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0cloudpickle>=0.2.2->swifter) (1.1.1)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.25.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (1.52.0)\n",
            "Collecting contextvars; python_version >= \"3.6\" and python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (4.2.1)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray>=1.0.0; extra == \"ray\"->modin[ray]>=0.8.1.1->swifter) (0.4.8)\n",
            "Building wheels for collected packages: swifter, gpustat, idna-ssl, contextvars\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.0.7-cp36-none-any.whl size=13977 sha256=e2910d6dc226c796f096d32041c5dfb0143561c3b4f3a9f5abd2901518bc7a18\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/58/39/5b59c5f4d66ce67bf55f0178e0940c964e89e9f60d70376a37\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-cp36-none-any.whl size=12622 sha256=5b09174c71687393e9254ae7822db11c00801d84bc2baef0e8896008bb1f2e6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=1bb95ec462d7c5967a3f869e827d8bb66c01ce3bfd60821442848ac7416bdf36\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7667 sha256=1e3ff5854fd67c5c7a25f694e1d7539c8ddabe5cf0896dbcf65f6b93f7ce13c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built swifter gpustat idna-ssl contextvars\n",
            "Installing collected packages: psutil, async-timeout, idna-ssl, multidict, yarl, aiohttp, colorama, blessings, gpustat, py-spy, hiredis, aioredis, colorful, redis, immutables, contextvars, opencensus-context, opencensus, aiohttp-cors, ray, pyarrow, modin, swifter, locket, partd, fsspec\n",
            "  Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed aiohttp-3.7.3 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 contextvars-2.4 fsspec-0.8.5 gpustat-0.6.0 hiredis-1.1.0 idna-ssl-1.1.0 immutables-0.14 locket-0.2.1 modin-0.8.3 multidict-5.1.0 opencensus-0.7.12 opencensus-context-0.1.2 partd-1.1.0 psutil-5.8.0 py-spy-0.3.4 pyarrow-1.0.0 ray-1.1.0 redis-3.5.3 swifter-1.0.7 yarl-1.6.3\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuSuDLKZUpYd"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EpQw4uSgeue",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "d5804eff-9aa7-42f0-e4a3-ae5a3a28be16"
      },
      "source": [
        "def remove_punct(text):\n",
        "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    return text\n",
        "\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = [word for word in re.split('\\W+', text) if word not in stopword]\n",
        "    return text\n",
        "\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "def stemming(text):\n",
        "    text = [ps.stem(word) for word in text]\n",
        "    return text\n",
        "\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemmatizer(text):\n",
        "    text = [wn.lemmatize(word) for word in text]\n",
        "    return ' '.join(text)\n",
        "\n",
        "def spell_correction(text):           # spelling correction\n",
        "    txt=TextBlob(text)\n",
        "    return txt.correct()\n",
        "\n",
        "def clean_data(x):\n",
        "    x=x.encode('ascii','ignore').decode() # remove texts other than english\n",
        "    x=re.sub('https*\\S+','',x) # remove urls\n",
        "    #x=spell_correction(x)\n",
        "    x=remove_punct(x) # remove punctuations\n",
        "    x=remove_stopwords(x) # remove stopwords\n",
        "    #x=stemming(x) # stemming\n",
        "    #x=lemmatizer(x) # lemmatization\n",
        "    return ' '.join(x)\n",
        "\n",
        "data=pd.read_excel('RNN-Data_2.xlsx',sheet_name=None)\n",
        "df_train=data['train data'].rename(columns={'utterance1':'text','intent':'label'})[['text','label']]\n",
        "df_test=pd.read_excel('GoldenCopyDataFile - IRIS.xlsx').rename(columns={'utterance':'text','intent':'label'})[['text','label']]\n",
        "#df_test=data['test data'].rename(columns={'input_conversation':'text','Corrected Intent':'label'})[['text','label']]\n",
        "df_test=df_test[df_test['label'].isin(df_train['label'])]\n",
        "df_train['text']=df_train['text'].swifter.apply(lambda x: clean_data(x))\n",
        "df_test['text']=df_test['text'].swifter.apply(lambda x: clean_data(x))\n",
        "df_train['text_length'] = df_train['text'].swifter.apply(lambda x: len(x.split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2d7bf34e7d93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstopword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6mTu1UyBOMX"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df_train['label'])\n",
        "df_train['label']=le.transform(df_train['label'])\n",
        "df_test['label']=df_test['label']=le.transform(df_test['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBDZNBTHUyFM"
      },
      "source": [
        "## Build the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "TNMHwIX5geue",
        "outputId": "300a1fd1-c11f-41c8-b90a-351f0159debc"
      },
      "source": [
        "#tokenization\n",
        "reviews=df_train.text\n",
        "tok = spacy.load('en')\n",
        "def tokenize (text):\n",
        "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
        "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
        "    nopunct = regex.sub(\" \", text.lower())\n",
        "    return [token.text for token in tok.tokenizer(nopunct)]\n",
        "\n",
        "from collections import Counter\n",
        "counts = Counter()\n",
        "for index, row in df_train.iterrows():\n",
        "    counts.update(tokenize(row['text']))\n",
        "#deleting infrequent words\n",
        "print(\"num_words before:\",len(counts.keys()))\n",
        "for word in list(counts):\n",
        "    if counts[word] < 2:\n",
        "        del counts[word]\n",
        "print(\"num_words after:\",len(counts.keys()))\n",
        "#creating vocabulary\n",
        "vocab2index = {\"\":0, \"UNK\":1}\n",
        "words = [\"\", \"UNK\"]\n",
        "for word in counts:\n",
        "    vocab2index[word] = len(words)\n",
        "    words.append(word)\n",
        "def encode_sentence(text, vocab2index, N=10):\n",
        "    tokenized = tokenize(text)\n",
        "    encoded = np.zeros(N, dtype=int)\n",
        "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
        "    length = min(N, len(enc1))\n",
        "    encoded[:length] = enc1[:length]\n",
        "    return encoded, length\n",
        "df_train['encoded'] = df_train['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
        "df_test['encoded'] = df_test['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_words before: 3719\n",
            "num_words after: 2208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_length</th>\n",
              "      <th>encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>external storage</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>[[2, 3, 4, 0, 0, 0, 0, 0, 0, 0], 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>z file opener</td>\n",
              "      <td>117</td>\n",
              "      <td>3</td>\n",
              "      <td>[[5, 6, 1, 0, 0, 0, 0, 0, 0, 0], 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How check Bank locked</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>[[7, 8, 9, 10, 0, 0, 0, 0, 0, 0], 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>msg file opened another program outlook</td>\n",
              "      <td>296</td>\n",
              "      <td>6</td>\n",
              "      <td>[[11, 6, 12, 13, 14, 15, 0, 0, 0, 0], 6]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>msg file opened Outlook</td>\n",
              "      <td>296</td>\n",
              "      <td>4</td>\n",
              "      <td>[[11, 6, 12, 15, 0, 0, 0, 0, 0, 0], 4]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      text  ...                                   encoded\n",
              "0                         external storage  ...       [[2, 3, 4, 0, 0, 0, 0, 0, 0, 0], 3]\n",
              "1                            z file opener  ...       [[5, 6, 1, 0, 0, 0, 0, 0, 0, 0], 3]\n",
              "2                    How check Bank locked  ...      [[7, 8, 9, 10, 0, 0, 0, 0, 0, 0], 4]\n",
              "3  msg file opened another program outlook  ...  [[11, 6, 12, 13, 14, 15, 0, 0, 0, 0], 6]\n",
              "4                  msg file opened Outlook  ...    [[11, 6, 12, 15, 0, 0, 0, 0, 0, 0], 4]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VGQRpswU4AM"
      },
      "source": [
        "## Build pytorch dataset and training and validataion functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry5yqGmmgeui"
      },
      "source": [
        "X = list(df_train['encoded'])\n",
        "y = list(df_train['label'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]\n",
        "\n",
        "train_ds = ReviewsDataset(X_train, y_train)\n",
        "valid_ds = ReviewsDataset(X_valid, y_valid)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_model(model, epochs=10, lr=0.001):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr) ### Adam optimizer\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        for x, y, l in train_dl:\n",
        "            x = x.long().to(device)\n",
        "            y = y.long().to(device)\n",
        "            y_pred = model(x, l)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(y_pred, y) # cross entropy loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "        val_loss, val_acc = validation_metrics(model, val_dl)\n",
        "        if i%5==0:\n",
        "          print(\"train loss %.3f, test loss %.3f, test accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
        "\n",
        "def validation_metrics (model, valid_dl):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    sum_rmse = 0.0\n",
        "    for x, y, l in valid_dl:\n",
        "        x = x.long().to(device)\n",
        "        y = y.long().to(device)\n",
        "        y_hat = model(x, l)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        pred = torch.max(y_hat, 1)[1]\n",
        "        correct += (pred == y).float().sum()\n",
        "        total += y.shape[0]\n",
        "        sum_loss += loss.item()*y.shape[0]\n",
        "        #sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
        "    return sum_loss/total, correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdJ8LstJJ8aY"
      },
      "source": [
        "batch_size = 1000\n",
        "vocab_size = len(words)\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ERwwAAaVAhP"
      },
      "source": [
        "## Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vesN1tGwgeuj"
      },
      "source": [
        "class LSTM_fixed_len(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True) # activation is taken as tanh\n",
        "        self.linear = nn.Linear(hidden_dim, 465)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x, l):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        lstm_out, (ht, ct) = self.lstm(x)\n",
        "        return self.linear(ht[-1])\n",
        "\n",
        "model = LSTM_fixed_len(vocab_size, 50, 500)\n",
        "model.to(device)\n",
        "train_model(model, epochs=100, lr=0.01)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhZpbFBlVS_1"
      },
      "source": [
        "## Glove pretrained embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZmF1nu-TmGl"
      },
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC0aoPMFgeuo"
      },
      "source": [
        "# def load_glove_vectors(glove_file=\"./glove.6B.50d.txt\"):\n",
        "#     \"\"\"Load the glove word vectors\"\"\"\n",
        "#     word_vectors = {}\n",
        "#     with open(glove_file) as f:\n",
        "#         for line in f:\n",
        "#             split = line.split()\n",
        "#             word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
        "#     return word_vectors\n",
        "\n",
        "# def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n",
        "#     \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
        "#     vocab_size = len(word_counts) + 2\n",
        "#     vocab_to_idx = {}\n",
        "#     vocab = [\"\", \"UNK\"]\n",
        "#     W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
        "#     W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
        "#     W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
        "#     vocab_to_idx[\"UNK\"] = 1\n",
        "#     i = 2\n",
        "#     for word in word_counts:\n",
        "#         if word in word_vecs:\n",
        "#             W[i] = word_vecs[word]\n",
        "#         else:\n",
        "#             W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
        "#         vocab_to_idx[word] = i\n",
        "#         vocab.append(word)\n",
        "#         i += 1   \n",
        "#     return W, np.array(vocab), vocab_to_idx\n",
        "# word_vecs = load_glove_vectors()\n",
        "# pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUC4369jVaBd"
      },
      "source": [
        "## Bidirectional LSTM with Glove embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcnG4Gy1geup"
      },
      "source": [
        "# class LSTM_glove_vecs(torch.nn.Module) :\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
        "#         super().__init__()\n",
        "#         self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "#         self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
        "#         self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
        "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "#         self.linear = nn.Linear(hidden_dim, 465)\n",
        "#         self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "#     def forward(self, x, l):\n",
        "#         x = self.embeddings(x)\n",
        "#         x = self.dropout(x)\n",
        "#         lstm_out, (ht, ct) = self.lstm(x)\n",
        "#         return self.linear(ht[-1])\n",
        "\n",
        "# model = LSTM_glove_vecs(vocab_size, 50, 500, pretrained_weights)\n",
        "# model.to(device)\n",
        "# train_model(model, epochs=50, lr=0.01)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K9cHd6MF2yE"
      },
      "source": [
        "# GRU with Glove embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xbmf4EqF18H"
      },
      "source": [
        "# class GRU_glove_vecs(torch.nn.Module) :\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
        "#         super().__init__()\n",
        "#         self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "#         self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
        "#         self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
        "#         self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "#         self.linear = nn.Linear(hidden_dim, 465)\n",
        "#         self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "#     def forward(self, x, l):\n",
        "#         x = self.embeddings(x)\n",
        "#         x = self.dropout(x)\n",
        "#         output, ht = self.gru(x)\n",
        "#         return self.linear(ht[-1])\n",
        "\n",
        "# model = GRU_glove_vecs(vocab_size, 50, 500, pretrained_weights)\n",
        "# train_model(model, epochs=5, lr=0.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuMABas_GPFV"
      },
      "source": [
        "# Prediction on Test data and calculation of accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD7RxuCeGObC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c324519c-8386-4aaf-c7ed-941798016fc3"
      },
      "source": [
        "X_test = list(df_test['encoded'])\n",
        "y_test = list(df_test['label'])\n",
        "\n",
        "test_ds = ReviewsDataset(X_test, y_test)\n",
        "test_dl = DataLoader(test_ds, batch_size=len(X_test))\n",
        "_,test_accuracy=validation_metrics(model,test_dl)\n",
        "test_accuracy=test_accuracy.cpu().detach().numpy().reshape(-1)[0]\n",
        "test_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2428712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yS-wa99Wdt-"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLWpYJaoN07_"
      },
      "source": [
        "x,y,l=next(iter(test_dl))\n",
        "x = x.long().to(device)\n",
        "y = y.long().to(device)\n",
        "y_pred = model(x, l)\n",
        "y_pred=y_pred.cpu().detach().numpy()\n",
        "y_pred=np.argmax(y_pred,axis=1)\n",
        "y_pred=le.inverse_transform(y_pred)\n",
        "df_test['Predictions']=y_pred\n",
        "df_test['Label']=le.inverse_transform(y_test)\n",
        "df_test[['text','Label','Predictions']].to_csv('Output.csv',index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAYS1ItmWMkE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}