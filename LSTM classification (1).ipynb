{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UaiSYVbrTXbq","executionInfo":{"status":"ok","timestamp":1613723140003,"user_tz":-480,"elapsed":19274,"user":{"displayName":"Dhanapal M","photoUrl":"","userId":"14519107343487747203"}},"outputId":"ba631b7b-436d-4c11-e34d-687574c8a362"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"35gDxPsaUgTx"},"source":["## Import libraries and read data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTOU0CEBgpDj","executionInfo":{"status":"ok","timestamp":1613720885550,"user_tz":-330,"elapsed":2516,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"86d2b3f7-6089-440f-fae6-8ef11e6428f3"},"source":["import os\n","#os.chdir('/content/drive/MyDrive/Dan/Dan')\n","os.chdir('/content/drive/MyDrive/Work/Dan')\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","#library imports\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","from textblob import TextBlob\n","import re\n","import spacy\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import string\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from sklearn.metrics import mean_squared_error\n","from sklearn.feature_extraction.text import CountVectorizer\n","import nltk \n","import string\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SuSuDLKZUpYd"},"source":["## Data Cleaning Functions"]},{"cell_type":"code","metadata":{"id":"_EpQw4uSgeue"},"source":["def remove_punct(text):\n","    text  = \"\".join([char for char in text if char not in string.punctuation])\n","    text = re.sub('[0-9]+', '', text)\n","    return text\n","\n","stopword = nltk.corpus.stopwords.words('english')\n","\n","def remove_stopwords(text):\n","    text = [word for word in re.split('\\W+', text) if word not in stopword]\n","    return text\n","\n","ps = nltk.PorterStemmer()\n","\n","def stemming(text):\n","    text = [ps.stem(word) for word in text]\n","    return text\n","\n","wn = nltk.WordNetLemmatizer()\n","\n","def lemmatizer(text):\n","    text = [wn.lemmatize(word) for word in text]\n","    return ' '.join(text)\n","\n","def spell_correction(text):           # spelling correction\n","    txt=TextBlob(text)\n","    return txt.correct()\n","\n","def clean_data(x):\n","    x=x.lower()\n","    x=x.encode('ascii','ignore').decode() # remove texts other than english\n","    x=re.sub('https*\\S+','',x) # remove urls\n","    #x=spell_correction(x)\n","    x=remove_punct(x) # remove punctuations\n","    x=remove_stopwords(x) # remove stopwords\n","    #x=stemming(x) # stemming\n","    #x=lemmatizer(x) # lemmatization\n","    return ' '.join(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YfvU2HBfnRtU"},"source":["### Prepare Train Data"]},{"cell_type":"code","metadata":{"id":"26zP_XcCnMH-"},"source":["data=pd.read_excel('RNN-Data_2.xlsx',sheet_name=None)\n","df_train=data['train data'].rename(columns={'utterance':'text','intent':'label'})[['text','label']]\n","df_train['text']=df_train['text'].apply(lambda x: clean_data(x))\n","df_train['text_length'] = df_train['text'].apply(lambda x: len(x.split()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"c_uaEpn6ngPc","executionInfo":{"status":"ok","timestamp":1613720887097,"user_tz":-330,"elapsed":4036,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"51ddddcb-3194-410f-9d3f-2f8af1f3966f"},"source":["df_train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","      <th>text_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>external storage</td>\n","      <td>access_management.access_to_file_storage_sites...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>z file opener</td>\n","      <td>computer.how_to_decrypt_a_file</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>check bank locked</td>\n","      <td>access_management.unlock_1bank_account</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>msg file opened another program outlook</td>\n","      <td>microsoft_office_365.msg_file</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>msg file opened outlook</td>\n","      <td>microsoft_office_365.msg_file</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                      text  ... text_length\n","0                         external storage  ...           2\n","1                            z file opener  ...           3\n","2                        check bank locked  ...           3\n","3  msg file opened another program outlook  ...           6\n","4                  msg file opened outlook  ...           4\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36K8jLBMoYQt","executionInfo":{"status":"ok","timestamp":1613720887099,"user_tz":-330,"elapsed":4025,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"b4cd0f33-5216-446e-a6dd-b1b31c0dd3aa"},"source":["df_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(19243, 3)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"VBDZNBTHUyFM"},"source":["## Build the vocabulary"]},{"cell_type":"code","metadata":{"id":"E6mTu1UyBOMX"},"source":["from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","le.fit(df_train['label'])\n","df_train['label']=le.transform(df_train['label'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TNMHwIX5geue","executionInfo":{"status":"ok","timestamp":1613720890853,"user_tz":-330,"elapsed":7762,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"7904c46d-2deb-435c-969d-d9e6a9092cc9"},"source":["#tokenization\n","reviews=df_train.text\n","tok = spacy.load('en')\n","def tokenize (text):\n","    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n","    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n","    nopunct = regex.sub(\" \", text.lower())\n","    return [token.text for token in tok.tokenizer(nopunct)]\n","\n","from collections import Counter\n","counts = Counter()\n","for index, row in df_train.iterrows():\n","    counts.update(tokenize(row['text']))\n","#deleting infrequent words\n","print(\"num_words before:\",len(counts.keys()))\n","for word in list(counts):\n","    if counts[word] < 2:\n","        del counts[word]\n","print(\"num_words after:\",len(counts.keys()))\n","#creating vocabulary\n","vocab2index = {\"\":0, \"UNK\":1}\n","words = [\"\", \"UNK\"]\n","for word in counts:\n","    vocab2index[word] = len(words)\n","    words.append(word)\n","def encode_sentence(text, vocab2index, N=10):\n","    tokenized = tokenize(text)\n","    encoded = np.zeros(N, dtype=int)\n","    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n","    length = min(N, len(enc1))\n","    encoded[:length] = enc1[:length]\n","    return encoded, length\n","df_train['encoded'] = df_train['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["num_words before: 3665\n","num_words after: 2165\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9VGQRpswU4AM"},"source":["## Build pytorch dataset and training and validataion functions."]},{"cell_type":"code","metadata":{"id":"Ry5yqGmmgeui"},"source":["X = list(df_train['encoded'])\n","y = list(df_train['label'])\n","from sklearn.model_selection import train_test_split\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n","\n","class ReviewsDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.y = Y\n","        \n","    def __len__(self):\n","        return len(self.y)\n","    \n","    def __getitem__(self, idx):\n","        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]\n","\n","train_ds = ReviewsDataset(X_train, y_train)\n","valid_ds = ReviewsDataset(X_valid, y_valid)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def train_model(model, epochs=10, lr=0.001):\n","    parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    optimizer = torch.optim.Adam(parameters, lr=lr) ### Adam optimizer\n","    for i in range(epochs):\n","        model.train()\n","        sum_loss = 0.0\n","        total = 0\n","        for x, y, l in train_dl:\n","            x = x.long().to(device)\n","            y = y.long().to(device)\n","            y_pred = model(x, l)\n","            optimizer.zero_grad()\n","            loss = F.cross_entropy(y_pred, y) # cross entropy loss\n","            loss.backward()\n","            optimizer.step()\n","            sum_loss += loss.item()*y.shape[0]\n","            total += y.shape[0]\n","        val_loss, val_acc = validation_metrics(model, val_dl)\n","        if i%5==0:\n","          print(\"train loss %.3f, test loss %.3f, test accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n","\n","def validation_metrics (model, valid_dl):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    sum_loss = 0.0\n","    sum_rmse = 0.0\n","    for x, y, l in valid_dl:\n","        x = x.long().to(device)\n","        y = y.long().to(device)\n","        y_hat = model(x, l)\n","        loss = F.cross_entropy(y_hat, y)\n","        pred = torch.max(y_hat, 1)[1]\n","        correct += (pred == y).float().sum()\n","        total += y.shape[0]\n","        sum_loss += loss.item()*y.shape[0]\n","        #sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n","    return sum_loss/total, correct/total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wdJ8LstJJ8aY"},"source":["batch_size = 1000\n","vocab_size = len(words)\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","val_dl = DataLoader(valid_ds, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ERwwAAaVAhP"},"source":["## Bidirectional LSTM"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vesN1tGwgeuj","executionInfo":{"status":"ok","timestamp":1613721295457,"user_tz":-330,"elapsed":36942,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"35655779-b248-4ede-b185-a8dcaaf36013"},"source":["class LSTM_fixed_len(torch.nn.Module) :\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n","        super().__init__()\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True) # activation is taken as tanh\n","        self.linear = nn.Linear(hidden_dim, df_train.label.nunique())\n","        self.dropout = nn.Dropout(0.2)\n","        \n","    def forward(self, x, l):\n","        x = self.embeddings(x)\n","        x = self.dropout(x)\n","        lstm_out, (ht, ct) = self.lstm(x)\n","        return self.linear(ht[-1])\n","\n","model = LSTM_fixed_len(vocab_size, 50, 500)\n","model.to(device)\n","train_model(model, epochs=100, lr=0.01)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train loss 6.040, test loss 5.572, test accuracy 0.036\n","train loss 1.056, test loss 0.980, test accuracy 0.757\n","train loss 0.232, test loss 0.592, test accuracy 0.868\n","train loss 0.125, test loss 0.633, test accuracy 0.866\n","train loss 0.088, test loss 0.643, test accuracy 0.861\n","train loss 0.077, test loss 0.653, test accuracy 0.878\n","train loss 0.068, test loss 0.624, test accuracy 0.878\n","train loss 0.059, test loss 0.657, test accuracy 0.874\n","train loss 0.060, test loss 0.692, test accuracy 0.877\n","train loss 0.055, test loss 0.682, test accuracy 0.874\n","train loss 0.061, test loss 0.645, test accuracy 0.882\n","train loss 0.052, test loss 0.656, test accuracy 0.885\n","train loss 0.050, test loss 0.700, test accuracy 0.878\n","train loss 0.056, test loss 0.693, test accuracy 0.879\n","train loss 0.052, test loss 0.719, test accuracy 0.882\n","train loss 0.050, test loss 0.711, test accuracy 0.879\n","train loss 0.055, test loss 0.706, test accuracy 0.881\n","train loss 0.054, test loss 0.660, test accuracy 0.883\n","train loss 0.051, test loss 0.722, test accuracy 0.878\n","train loss 0.056, test loss 0.737, test accuracy 0.877\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LhZpbFBlVS_1"},"source":["## Glove pretrained embedding vector"]},{"cell_type":"code","metadata":{"id":"KZmF1nu-TmGl"},"source":["#!wget http://nlp.stanford.edu/data/glove.6B.zip\n","#!unzip glove*.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CC0aoPMFgeuo"},"source":["# def load_glove_vectors(glove_file=\"./glove.6B.50d.txt\"):\n","#     \"\"\"Load the glove word vectors\"\"\"\n","#     word_vectors = {}\n","#     with open(glove_file) as f:\n","#         for line in f:\n","#             split = line.split()\n","#             word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n","#     return word_vectors\n","\n","# def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n","#     \"\"\" Creates embedding matrix from word vectors\"\"\"\n","#     vocab_size = len(word_counts) + 2\n","#     vocab_to_idx = {}\n","#     vocab = [\"\", \"UNK\"]\n","#     W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n","#     W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n","#     W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n","#     vocab_to_idx[\"UNK\"] = 1\n","#     i = 2\n","#     for word in word_counts:\n","#         if word in word_vecs:\n","#             W[i] = word_vecs[word]\n","#         else:\n","#             W[i] = np.random.uniform(-0.25,0.25, emb_size)\n","#         vocab_to_idx[word] = i\n","#         vocab.append(word)\n","#         i += 1   \n","#     return W, np.array(vocab), vocab_to_idx\n","# word_vecs = load_glove_vectors()\n","# pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUC4369jVaBd"},"source":["## Bidirectional LSTM with Glove embedding"]},{"cell_type":"code","metadata":{"id":"xcnG4Gy1geup"},"source":["# class LSTM_glove_vecs(torch.nn.Module) :\n","#     def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n","#         super().__init__()\n","#         self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","#         self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n","#         self.embeddings.weight.requires_grad = False ## freeze embeddings\n","#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n","#         self.linear = nn.Linear(hidden_dim, 465)\n","#         self.dropout = nn.Dropout(0.2)\n","        \n","#     def forward(self, x, l):\n","#         x = self.embeddings(x)\n","#         x = self.dropout(x)\n","#         lstm_out, (ht, ct) = self.lstm(x)\n","#         return self.linear(ht[-1])\n","\n","# model = LSTM_glove_vecs(vocab_size, 50, 500, pretrained_weights)\n","# model.to(device)\n","# train_model(model, epochs=50, lr=0.01)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6K9cHd6MF2yE"},"source":["# GRU with Glove embedding"]},{"cell_type":"code","metadata":{"id":"7Xbmf4EqF18H"},"source":["# class GRU_glove_vecs(torch.nn.Module) :\n","#     def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n","#         super().__init__()\n","#         self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","#         self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n","#         self.embeddings.weight.requires_grad = False ## freeze embeddings\n","#         self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n","#         self.linear = nn.Linear(hidden_dim, 465)\n","#         self.dropout = nn.Dropout(0.2)\n","        \n","#     def forward(self, x, l):\n","#         x = self.embeddings(x)\n","#         x = self.dropout(x)\n","#         output, ht = self.gru(x)\n","#         return self.linear(ht[-1])\n","\n","# model = GRU_glove_vecs(vocab_size, 50, 500, pretrained_weights)\n","# train_model(model, epochs=5, lr=0.1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uuMABas_GPFV"},"source":["# Prediction on Test data and calculation of accuracy"]},{"cell_type":"markdown","metadata":{"id":"mHtiqXRLzdlz"},"source":["### Accuracy calculation function"]},{"cell_type":"code","metadata":{"id":"mmcYp32Wzbr_"},"source":["df_train=data['train data'].rename(columns={'utterance':'text','intent':'label'})[['text','label']]\n","def prep_data(df_test):\n","  df_test['text']=df_test['text'].apply(lambda x: clean_data(x))\n","  df_test=df_test[df_test['label'].isin(df_train['label'])] # take only matching rows from train and test data\n","  df_test['label']=le.transform(df_test['label'])\n","  df_test['encoded'] = df_test['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n","  return df_test\n","\n","def calc_acc(df_test): \n","  try: \n","    X_test = list(df_test['encoded'])\n","    y_test = list(df_test['label'])\n","    test_ds = ReviewsDataset(X_test, y_test)\n","    test_dl = DataLoader(test_ds, batch_size=len(X_test))\n","    _,test_accuracy=validation_metrics(model,test_dl)\n","    test_accuracy=test_accuracy.cpu().detach().numpy().reshape(-1)[0]\n","    return test_accuracy\n","  except:\n","    print('Cannot Calculate the accuracy as test data labels are completely different from train data.')\n","\n","def predictions(df_test,model,le,filename):\n","  try:\n","    X_test = list(df_test['encoded'])\n","    y_test = list(df_test['label'])\n","    test_ds = ReviewsDataset(X_test, y_test)\n","    test_dl = DataLoader(test_ds, batch_size=len(X_test))\n","    x,y,l=next(iter(test_dl))\n","    x = x.long().to(device)\n","    y = y.long().to(device)\n","    y_pred = model(x, l)\n","    y_pred=y_pred.cpu().detach().numpy()\n","    y_pred=np.argmax(y_pred,axis=1)\n","    y_pred=le.inverse_transform(y_pred)\n","    df_test['Predictions']=y_pred\n","    df_test['Label']=le.inverse_transform(y_test)\n","    df_test[['text','Label','Predictions']].to_csv(filename,index=None)\n","    print('The predictions are saved as {}'.format(filename))\n","  except:\n","    print('Cannot Calculate the predictions as test data labels are completely different from train data.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkctTWRlxs7v"},"source":["### Calculate accuracies on test datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPc39i-hznJM","executionInfo":{"status":"ok","timestamp":1613721010169,"user_tz":-330,"elapsed":1191,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"5bbaa556-801a-434a-fc7c-0906a2a426ae"},"source":["df_test_unseen_iris=pd.read_excel('Unseen - IRIS.xlsx').rename(columns={'input_conversation':'text','Corrected Intent':'label'})[['text','label']]\n","df_test_unseen_iris=prep_data(df_test_unseen_iris)\n","calc_acc(df_test_unseen_iris)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.24090463"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbMN9c1201-b","executionInfo":{"status":"ok","timestamp":1613721012813,"user_tz":-330,"elapsed":1568,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"f2f39577-fd7a-4b64-d8b0-b8480ae2d00e"},"source":["df_test_golden_iris=pd.read_excel('GoldenCopyDataFile - IRIS.xlsx').rename(columns={'utterance':'text','intent':'label'})[['text','label']]\n","df_test_golden_iris=prep_data(df_test_golden_iris)\n","calc_acc(df_test_golden_iris)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8683955"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H9RUweat02HI","executionInfo":{"status":"ok","timestamp":1613721014759,"user_tz":-330,"elapsed":1189,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"928ab252-b71e-433a-bc98-4fe6d662469c"},"source":["df_test_golden_hiri=pd.read_excel('GoldenCopyDataFile - HIRI.xlsx').rename(columns={'utterance':'text','intent':'label'})[['text','label']]\n","df_test_golden_hiri=prep_data(df_test_golden_hiri)\n","calc_acc(df_test_golden_hiri)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cannot Calculate the accuracy as test data labels are completely different from train data.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Wc24LBm0113","executionInfo":{"status":"ok","timestamp":1613721018052,"user_tz":-330,"elapsed":1231,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"949923e0-b3a8-4595-f05d-b42f9b8f648a"},"source":["df_test_unseen_hiri=pd.read_excel('Unseen data - HIRI.xlsx').rename(columns={'Intent':'label','Input Conversation':'text'})[['text','label']]\n","df_test_unseen_hiri=prep_data(df_test_unseen_hiri)\n","calc_acc(df_test_unseen_hiri)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cannot Calculate the accuracy as test data labels are completely different from train data.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3yS-wa99Wdt-"},"source":["# Predictions"]},{"cell_type":"code","metadata":{"id":"sLWpYJaoN07_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613721101203,"user_tz":-330,"elapsed":1061,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"a892946b-e904-46b4-eb92-f8e9dd6d53ff"},"source":["predictions(df_test_unseen_iris,model,le,'OutputunseenIRIS_out.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The predictions are saved as OutputunseenIRIS_out.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fu3C_6eUkPnO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613721124037,"user_tz":-330,"elapsed":1037,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"4d6346fe-0bc4-4aa8-9932-afb0e37ebb78"},"source":["predictions(df_test_golden_iris,model,le,'OutputgoldenIRIS_out.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The predictions are saved as OutputgoldenIRIS_out.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Rc3k6rzRz2h","executionInfo":{"status":"ok","timestamp":1613721211621,"user_tz":-330,"elapsed":991,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"f139f6d1-2463-4d9e-92f2-313c8dc3fc17"},"source":["predictions(df_test_golden_hiri,model,le,'OutputgoldenHIRI_out.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cannot Calculate the predictions as test data labels are completely different from train data.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LZPA6uqmR0AA","executionInfo":{"status":"ok","timestamp":1613721233650,"user_tz":-330,"elapsed":1096,"user":{"displayName":"bharat bhushan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64","userId":"13761418364707206314"}},"outputId":"78ffa521-dcb1-48a4-a144-ccaed31d2066"},"source":["predictions(df_test_unseen_hiri,model,le,'OutputunseenHIRI_out.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cannot Calculate the predictions as test data labels are completely different from train data.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5RFGd8VJSZcd"},"source":["## The Accuracies and predictions fro HIRI datasets could not be calculated since the test data is completely different from the train data."]},{"cell_type":"code","metadata":{"id":"8UZWLF8FR-Rs"},"source":[""],"execution_count":null,"outputs":[]}]}