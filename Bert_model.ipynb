{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as nnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBaseUncased(tez.Model):\n",
    "    def __init__(self, num_train_steps, num_classes):\n",
    "        super().__init__()\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"/nlv/TrainBertDhana/intentpredid/bert-base-multilingual-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"/nlv/TrainBertDhana/intentpredid/bert-base-multilingual-uncased\")\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, num_classes)\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.step_scheduler_after = \"batch\"\n",
    "    def fetch_optimizer(self):\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        opt = AdamW(optimizer_parameters, lr=3e-5)\n",
    "        return opt\n",
    "    def fetch_scheduler(self):\n",
    "        sch = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=0, num_training_steps=self.num_train_steps\n",
    "        )\n",
    "        return sch\n",
    "    def loss(self, outputs, targets):\n",
    "        if targets is None:\n",
    "            return None\n",
    "        return nn.CrossEntropyLoss()(outputs, targets)\n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        if targets is None:\n",
    "            return {}\n",
    "        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        return {\"accuracy\": accuracy}\n",
    "    def forward(self, ids, mask, token_type_ids, targets=None):\n",
    "        _, o_2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        b_o = self.bert_drop(o_2)\n",
    "        output = self.out(b_o)\n",
    "        loss = self.loss(output, targets)\n",
    "        acc = self.monitor_metrics(output, targets)\n",
    "        return output, loss, acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "DEVICE = \"cuda\"\n",
    "MAX_LEN = 64\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "BERT_PATH = \"/nlv/TrainBertDhana/intentpredid/bert-base-multilingual-uncased\"\n",
    "MODEL_PATH = \"model_fold_txr.bin\"\n",
    "TRAINING_FILE = \"/root/docker_data/train.csv\"\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "MODEL = BERTBaseUncased(5800,464)\n",
    "MODEL.load_state_dict(torch.load(\"/nlv/TrainBertDhana/intentpredid/model_fold_txr.bin\"))\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = config.DEVICE\n",
    "# PREDICTION_DICT = dict()\n",
    "tokenizer = TOKENIZER\n",
    "max_len = MAX_LEN\n",
    "# sent_csv=pd.read_csv(\"dfx_valid_fold1.csv\",usecols=['text', 'category'])\n",
    "sent_csv=pd.read_excel(\"RNN-Data_2.xlsx\",sheet_name=\"Test\",usecols=['text', 'category'])\n",
    "sentence=sent_csv.text.values\n",
    "# sentence=[sentence[0]]\n",
    "\n",
    "data=pd.read_excel(\"RNN-Data_2.xlsx\",sheet_name=\"Test\")\n",
    "dmap=pd.read_excel(\"RNN-Data_2.xlsx\",sheet_name=\"mapping\")\n",
    "\n",
    "# d= dict([(i,a) for i, a in zip(dmap.Intent, dmap.mapping)])\n",
    "# print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= dict([(i,a) for i, a in zip(dmap.mapping, dmap.intent)])\n",
    "print(d)\n",
    "maxindx,predictions,maxindx1,maxindx2,maxindx3,maxindx4,predictions1,predictions2,predictions3,predictions4=[],[],[],[],[],[],[],[],[],[]\n",
    "top_p1,top_p2,top_p3,top_class1,top_class2,top_class3=[],[],[],[],[],[]\n",
    "for index,value in enumerate(sentence):\n",
    "    print(value)\n",
    "    review = str(value)\n",
    "    review = \" \".join(review.split())\n",
    "    inputs = tokenizer.encode_plus(\n",
    "            review, None, add_special_tokens=True, max_length=max_len\n",
    "        )\n",
    "    ids = inputs[\"input_ids\"]\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "    padding_length = max_len - len(ids)\n",
    "    ids = ids + ([0] * padding_length)\n",
    "    mask = mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n",
    "    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n",
    "    ids = ids.to(DEVICE, dtype=torch.long)\n",
    "    token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n",
    "    mask = mask.to(DEVICE, dtype=torch.long)\n",
    "    outputs = MODEL(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "    maxidx=torch.argmax(outputs[0],keepdim=False)\n",
    "    maxindx.append(int(maxidx))\n",
    "    print(int(maxidx))\n",
    "    \n",
    "    # for intent, cat in d.items():\n",
    "        # if cat == int(maxidx):\n",
    "            # print(intent)\n",
    "    \n",
    "    print(d[int(maxidx)])  \n",
    "    predictions.append(d[int(maxidx)])\n",
    "    # outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "    print(str(outputs[0]))\n",
    "    \n",
    "    prob = nnf.softmax(outputs[0], dim=1)\n",
    "    top_p, top_class = prob.topk(3, dim = 1)\n",
    "    print(\"confidence score\",top_p)\n",
    "    print(\"confidence score1\",top_p[0][0])\n",
    "    \n",
    "    top_p1.append(float(top_p[0][0]))\n",
    "    top_p2.append(float(top_p[0][1]))\n",
    "    top_p3.append(float(top_p[0][2]))\n",
    "    print(\"top_class\",top_class)\n",
    "    print(\"top_class1\",top_class[0][0])\n",
    "    \n",
    "    top_class1.append(int(top_class[0][0]))\n",
    "    top_class2.append(int(top_class[0][1]))\n",
    "    top_class3.append(int(top_class[0][2]))\n",
    "    \n",
    "    predictions1.append(d[int(top_class[0][0])])\n",
    "    predictions2.append(d[int(top_class[0][1])])\n",
    "    predictions3.append(d[int(top_class[0][2])])\n",
    "    \n",
    "sent_csv[\"pred_idx\"]=pd.DataFrame(maxindx)\n",
    "sent_csv[\"pred_values\"]=pd.DataFrame(predictions)\n",
    "sent_csv[\"top_p1\"]=pd.DataFrame(top_p1)\n",
    "sent_csv[\"top_p2\"]=pd.DataFrame(top_p2)\n",
    "sent_csv[\"top_p3\"]=pd.DataFrame(top_p3)\n",
    "sent_csv[\"top_class1\"]=pd.DataFrame(top_class1)\n",
    "sent_csv[\"top_class2\"]=pd.DataFrame(top_class2)\n",
    "sent_csv[\"top_class3\"]=pd.DataFrame(top_class3)\n",
    "sent_csv[\"predictions1\"]=pd.DataFrame(predictions1)\n",
    "sent_csv[\"predictions2\"]=pd.DataFrame(predictions2)\n",
    "sent_csv[\"predictions3\"]=pd.DataFrame(predictions3)\n",
    "\n",
    "sent_csv.to_csv(\"sentcsv.csv\")\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
