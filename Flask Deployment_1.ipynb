{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9ERl44YU1GD"
   },
   "source": [
    "## Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4IA9htCU3kv"
   },
   "outputs": [],
   "source": [
    "# This is dummy connection code which has to be changed based on database types and file names\n",
    "\n",
    "# import mysql.connector\n",
    "# mydb = mysql.connector.connect(\n",
    "#   host=\"localhost\",\n",
    "#   user=\"yourusername\",\n",
    "#   password=\"yourpassword\"\n",
    "# )\n",
    "\n",
    "# mycursor = mydb.cursor()\n",
    "# mycursor.execute(\"SELECT * FROM data\")\n",
    "# data = mycursor.fetchall()\n",
    "\n",
    "# print(mydb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and create the model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10465,
     "status": "ok",
     "timestamp": 1614227135087,
     "user": {
      "displayName": "bharat bhushan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhcX3S2BJK7Bcvr4yB9N8JqhPDzE-u0lVrSU0pQeA=s64",
      "userId": "13761418364707206314"
     },
     "user_tz": -330
    },
    "id": "mw9vqhokbRtr",
    "outputId": "7a39a7a7-15cc-474c-a470-ff5bdcb97613"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if not os.path.exists('model_files'):\n",
    "    os.mkdir('model_files/')\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in re.split('\\W+', text) if word not in stopword]\n",
    "    return text\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def spell_correction(text):           # spelling correction\n",
    "    txt=TextBlob(text)\n",
    "    return txt.correct()\n",
    "\n",
    "def clean_data(x):\n",
    "    x=x.lower()\n",
    "    x=x.encode('ascii','ignore').decode() # remove texts other than english\n",
    "    x=re.sub('https*\\S+','',x) # remove urls\n",
    "    #x=spell_correction(x)\n",
    "    x=remove_punct(x) # remove punctuations\n",
    "    x=remove_stopwords(x) # remove stopwords\n",
    "    #x=stemming(x) # stemming\n",
    "    #x=lemmatizer(x) # lemmatization\n",
    "    return ' '.join(x)\n",
    "    \n",
    "\n",
    "### Load the training data\n",
    "data=pd.read_excel('RNN-Data_2.xlsx',sheet_name=None)\n",
    "df_train=data['train data'].rename(columns={'utterance':'text','intent':'label'})[['text','label']]\n",
    "df_train['text']=df_train['text'].apply(lambda x: clean_data(x))\n",
    "df_train['text_length'] = df_train['text'].apply(lambda x: len(x.split()))\n",
    "import pickle\n",
    "with open('model_files/labels.pkl', 'wb') as handle:\n",
    "    pickle.dump(df_train.label.unique().tolist(), handle)\n",
    "\n",
    "### Build the vocabulary\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_train['label'])\n",
    "df_train['label']=le.transform(df_train['label'])\n",
    "with open('model_files/label_encoder.pkl', 'wb') as handle:\n",
    "    pickle.dump(le, handle)\n",
    "\n",
    "reviews=df_train.text\n",
    "tok = spacy.load('en')\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter()\n",
    "for index, row in df_train.iterrows():\n",
    "    counts.update(tokenize(row['text']))\n",
    "#deleting infrequent words\n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))\n",
    "#creating vocabulary\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)\n",
    "\n",
    "with open('model_files/vocab.pkl','wb') as vocab:\n",
    "    pickle.dump(vocab2index,vocab)\n",
    "    \n",
    "def encode_sentence(text, vocab2index, N=10):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length\n",
    "df_train['encoded'] = df_train['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
    "\n",
    "### Build training and validation functions\n",
    "X = list(df_train['encoded'])\n",
    "y = list(df_train['label'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]\n",
    "\n",
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr) ### Adam optimizer\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dl:\n",
    "            x = x.long().to(device)\n",
    "            y = y.long().to(device)\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y) # cross entropy loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc = validation_metrics(model, val_dl)\n",
    "        if i%5==0:\n",
    "          print(\"train loss %.3f, test loss %.3f, test accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in valid_dl:\n",
    "        x = x.long().to(device)\n",
    "        y = y.long().to(device)\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        #sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total\n",
    "\n",
    "batch_size = 1000\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)\n",
    "\n",
    "class LSTM_fixed_len(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True) # activation is taken as tanh\n",
    "        self.linear = nn.Linear(hidden_dim, df_train.label.nunique())\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])\n",
    "\n",
    "model = LSTM_fixed_len(vocab_size, 50, 500)\n",
    "model.to(device)\n",
    "train_model(model, epochs=50, lr=0.01)\n",
    "\n",
    "torch.save(model, 'model_files/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from flask import Flask\n",
    "from string import Template\n",
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "tok = spacy.load('en')\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in re.split('\\W+', text) if word not in stopword]\n",
    "    return text\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def spell_correction(text):           # spelling correction\n",
    "    txt=TextBlob(text)\n",
    "    return txt.correct()\n",
    "\n",
    "def clean_data(x):\n",
    "    x=x.lower()\n",
    "    x=x.encode('ascii','ignore').decode() # remove texts other than english\n",
    "    x=re.sub('https*\\S+','',x) # remove urls\n",
    "    #x=spell_correction(x)\n",
    "    x=remove_punct(x) # remove punctuations\n",
    "    x=remove_stopwords(x) # remove stopwords\n",
    "    #x=stemming(x) # stemming\n",
    "    #x=lemmatizer(x) # lemmatization\n",
    "    return ' '.join(x)\n",
    "    \n",
    "tok = spacy.load('en')\n",
    "\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]\n",
    "\n",
    "def encode_sentence(text, vocab2index, N=10):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length\n",
    "    \n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]\n",
    "\n",
    "def prep_data(text):\n",
    "  text=clean_data(text)\n",
    "  text=np.array(encode_sentence(text,vocab2index))\n",
    "  return text\n",
    "\n",
    "class LSTM_fixed_len(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True) # activation is taken as tanh\n",
    "        self.linear = nn.Linear(hidden_dim, df_train.label.nunique())\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])\n",
    "import pickle\n",
    "with open('model_files/label_encoder.pkl', 'rb') as handle:\n",
    "    le= pickle.load(handle)\n",
    "with open('model_files/labels.pkl', 'rb') as handle:\n",
    "    labels=pickle.load(handle)\n",
    "model = torch.load('model_files/model.pth')\n",
    "\n",
    "with open('model_files/vocab.pkl','rb') as vocab:\n",
    "    vocab2index=pickle.load(vocab)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "\treturn render_template('home.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "  if request.method=='POST':\n",
    "    message = request.form['message']\n",
    "    x=torch.from_numpy(prep_data(message)[0].astype(np.int32)).reshape(1,-1).long().to(device)\n",
    "    pred=le.inverse_transform(np.argmax(model(x,None).cpu().detach().numpy(),axis=1))[0]   \n",
    "    print(pred) \n",
    "  return render_template('result.html',prediction = pred)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t#decide what port to run the app in\n",
    "\tport = int(os.environ.get('PORT', 5000))\n",
    "\t#run the app locally on the givn port\n",
    "\tapp.run(host='0.0.0.0', port=port)\n",
    "\t#optional if we want to run in debugging mode\n",
    "\t#app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOhOvExzHd9rFaoTrQDl/Ns",
   "collapsed_sections": [],
   "name": "Flask Deployment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
