{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hihp8VYCMP33"
      },
      "source": [
        "### Load the libraries (change directories based on the system on which code is running)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7O-Ek3TTqCs",
        "outputId": "f9586ff8-a034-418c-e9be-b229fde6c59f"
      },
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "#os.chdir('/content/drive/MyDrive/Dan/Dan')\n",
        "os.chdir('/content/drive/MyDrive/Work/Dan/BERT Model')\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "!pip install transformers\n",
        "#!pip install -U torchtext\n",
        "# Preliminaries\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Dswby7sx40"
      },
      "source": [
        "## Prepare Datasets (uncomment if datasets are changing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMhK-nWOT0w7"
      },
      "source": [
        "# train=pd.read_excel('Model_training_all_06032021.xlsx',sheet_name='Sheet2').drop_duplicates().reset_index(drop=True)\n",
        "# train=train[train['language']=='EN'][['cantonese_text','intent']].rename(columns={'cantonese_text':'text','intent':'label'})\n",
        "# test=pd.read_excel('Gold_data_all_06032021.xlsx',sheet_name='Sheet2').drop_duplicates().reset_index(drop=True)\n",
        "# test=test[test['language']=='EN'][['utterance','intentid_expected']].rename(columns={'utterance':'text','intentid_expected':'label'})\n",
        "# test=test[test.label.isin(train.label)]\n",
        "\n",
        "\n",
        "# def remove_punct(text):\n",
        "#     text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "#     text = re.sub('[0-9]+', '', text)\n",
        "#     return text\n",
        "\n",
        "# stopword = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# def remove_stopwords(text):\n",
        "#     text = [word for word in re.split('\\W+', text) if word not in stopword]\n",
        "#     return text\n",
        "\n",
        "# ps = nltk.PorterStemmer()\n",
        "\n",
        "# def stemming(text):\n",
        "#     text = [ps.stem(word) for word in text]\n",
        "#     return text\n",
        "\n",
        "# wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "# def lemmatizer(text):\n",
        "#     text = [wn.lemmatize(word) for word in text]\n",
        "#     return ' '.join(text)\n",
        "\n",
        "# def spell_correction(text):           # spelling correction\n",
        "#     txt=TextBlob(text)\n",
        "#     return txt.correct()\n",
        "\n",
        "# def clean_data(x):\n",
        "#     x=x.lower()\n",
        "#     x=x.encode('ascii','ignore').decode() # remove texts other than english\n",
        "#     x=re.sub('https*\\S+','',x) # remove urls\n",
        "#     #x=spell_correction(x)\n",
        "#     x=remove_punct(x) # remove punctuations\n",
        "#     x=remove_stopwords(x) # remove stopwords\n",
        "#     #x=stemming(x) # stemming\n",
        "#     #x=lemmatizer(x) # lemmatization\n",
        "#     return ' '.join(x)\n",
        "\n",
        "\n",
        "# train['text']=train['text'].apply(lambda x: clean_data(x))\n",
        "# test['text']=test['text'].apply(lambda x: clean_data(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0Yg22C7V6uM"
      },
      "source": [
        "# from sklearn import preprocessing\n",
        "# import pickle\n",
        "# le = preprocessing.LabelEncoder()\n",
        "# le.fit(train['label'])\n",
        "# train['label']=le.transform(train['label'])\n",
        "# test['label']=le.transform(test['label'])\n",
        "# with open('model_files/label_encoder.pkl', 'wb') as handle:\n",
        "#     pickle.dump(le, handle)\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# train,valid = train_test_split(train, test_size=0.2)\n",
        "# train.to_csv('train.csv',index=None)\n",
        "# test.to_csv('test.csv',index=None)\n",
        "# valid.to_csv('valid.csv',index=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY09898_MjD-"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iW4sLBpstBN"
      },
      "source": [
        "train=pd.read_csv('train.csv')\n",
        "test=pd.read_csv('test.csv')\n",
        "valid=pd.read_csv('valid.csv')\n",
        "\n",
        "with open('model_files/label_encoder.pkl', 'rb') as handle:\n",
        "    le=pickle.load(handle)\n",
        "train_text,train_labels=train['text'],train['label']\n",
        "val_text,val_labels=valid['text'],valid['label']\n",
        "test_text,test_labels=test['text'],test['label']\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "seq_len = [len(str(i).split()) for i in train_text]\n",
        "tokens_train = tokenizer.batch_encode_plus(train_text.astype(str).tolist(),max_length=25,truncation=True,pad_to_max_length=True)\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(val_text.astype(str).tolist(),max_length=25,truncation=True,pad_to_max_length=True)\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(test_text.astype(str).tolist(),max_length=25,truncation=True,pad_to_max_length=True)\n",
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYVMVwNoMlY6"
      },
      "source": [
        "## BERT model Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psuq_w7CG7V7"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 128\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi-KJmlAMq5O"
      },
      "source": [
        "## Pretrained BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlNuUW19G_KO"
      },
      "source": [
        "\n",
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,len(le.classes_))\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      k = self.bert(sent_id, attention_mask=mask)\n",
        "      cls_hs=k[1]\n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOJ7lL8gHEl6"
      },
      "source": [
        "\n",
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 0.001)          # learning rate\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss() \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbsqaauMy3M"
      },
      "source": [
        "## Training of BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO_qg60bHN5G"
      },
      "source": [
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  correct=0\n",
        "  total=0\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    correct += (torch.max(preds, 1)[1] == labels).float().sum()\n",
        "    total += labels.shape[0]\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds, correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIxBhz37M2me"
      },
      "source": [
        "## Evaluate the BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPspriySHVSP"
      },
      "source": [
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  correct=0\n",
        "  total=0\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "      correct += (torch.max(preds, 1)[1] == labels).float().sum()\n",
        "      total += labels.shape[0]\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds ,correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRaPXU5CM8Xc"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqLsSn5CHdTR",
        "outputId": "f0dadd32-a46a-43c0-acb0-519658123514"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _, train_acc = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _, valid_acc = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}, Training Accuracy: {train_acc: .3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}, Validation Accuracy: {valid_acc: .3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 6.823, Training Accuracy:  0.014\n",
            "Validation Loss: 6.715, Validation Accuracy:  0.014\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 6.634, Training Accuracy:  0.020\n",
            "Validation Loss: 6.506, Validation Accuracy:  0.021\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 6.432, Training Accuracy:  0.032\n",
            "Validation Loss: 6.255, Validation Accuracy:  0.041\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 6.182, Training Accuracy:  0.046\n",
            "Validation Loss: 5.964, Validation Accuracy:  0.058\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 5.940, Training Accuracy:  0.057\n",
            "Validation Loss: 5.708, Validation Accuracy:  0.070\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 5.739, Training Accuracy:  0.064\n",
            "Validation Loss: 5.465, Validation Accuracy:  0.086\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 5.565, Training Accuracy:  0.074\n",
            "Validation Loss: 5.297, Validation Accuracy:  0.095\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 5.408, Training Accuracy:  0.084\n",
            "Validation Loss: 5.114, Validation Accuracy:  0.119\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 5.263, Training Accuracy:  0.093\n",
            "Validation Loss: 4.934, Validation Accuracy:  0.128\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch   100  of    291.\n",
            "  Batch   200  of    291.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     73.\n",
            "\n",
            "Training Loss: 5.133, Training Accuracy:  0.103\n",
            "Validation Loss: 4.808, Validation Accuracy:  0.138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R1n_b8PATGWs",
        "outputId": "f1917a6a-6445-4ffa-e9ff-fde98c6c2b3c"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WKsqmXwSHgkb",
        "outputId": "e36c19e7-b82f-4ddb-9dc0-8f4abae77953"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()\n",
        "\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "test['predicted']=le.inverse_transform(preds)\n",
        "test['actual']=le.inverse_transform(test['label'])\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(test['actual'], test['predicted'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07214345287739783"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7rQ0Y0tiXewr"
      },
      "source": [
        "test.to_csv('predicted.csv',index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "osHQsv4DXw8r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}